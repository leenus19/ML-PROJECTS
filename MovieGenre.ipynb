{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a2270b-204c-428b-931c-7a853629db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b6cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_file_path = 'train_data.txt'\n",
    "train_df = pd.read_csv(train_file_path, delimiter=':::', engine='python', header=None, names=['ID', 'TITLE', 'GENRE', 'DESCRIPTION'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba401f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_file_path = 'test_data.txt'\n",
    "test_df = pd.read_csv(test_file_path, delimiter=':::', engine='python', header=None, names=['ID', 'TITLE', 'DESCRIPTION'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce26004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: fill missing values, encode categorical variables\n",
    "# Add more code here for pre-processing and feature engineering if needed\n",
    "\n",
    "# Prepare data for machine learning\n",
    "X_train, y_train = train_df['DESCRIPTION'], train_df['GENRE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc60d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356982bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization for training and validation sets\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0df349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model (Logistic Regression)\n",
    "genre_model = LogisticRegression()\n",
    "genre_model.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f71969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "genre_pred = genre_model.predict(X_val_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "accuracy = accuracy_score(y_val, genre_pred)\n",
    "conf_matrix = confusion_matrix(y_val, genre_pred)\n",
    "classification_rep = classification_report(y_val, genre_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b92365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Validation Set: 0.5788988287374343\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  67    0    1    0    0   29    1   34   93    0    0    0    0   11\n",
      "     0    0    0    0    0    0    4    7    4    0   12    0    0]\n",
      " [   0   24   13    0    0   32    0    6   27    0    0    0    0    1\n",
      "     0    0    0    0    0    0    0    7    0    0    1    0    1]\n",
      " [   5    0   21    0    0   18    0   28   35    1    0    0    0   10\n",
      "     0    0    0    0    1    0    4   10    0    0    3    0    3]\n",
      " [   1    0    2   10    0   22    0   17   23    8    1    0    0    2\n",
      "     1    0    0    0    0    0    6   11    0    0    0    0    0]\n",
      " [   0    0    0    0    0    3    0   38   16    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    4    0    0    0    0    0]\n",
      " [   6    1    1    1    0  832    1   87  429    2    0    0    0   12\n",
      "     2    1    0    0    7    1    1   49    2    1    4    0    3]\n",
      " [   7    0    0    0    0   19    3   13   48    0    0    0    0    6\n",
      "     0    0    0    0    0    0    1    5    0    0    5    0    0]\n",
      " [   3    1    1    0    0   61    0 2257  199    0    0    0    0    9\n",
      "    20    0    0    0    7    0    1   95    1    0    3    0    1]\n",
      " [   8    2    2    0    0  235    1  227 2105    2    0    0    0   15\n",
      "     0    0    0    0    1    4    0   81    2    0   11    0    1]\n",
      " [   1    0    2    2    0   34    0   31   49   10    0    0    0    1\n",
      "     4    0    0    0    2    0    0   12    1    1    0    0    0]\n",
      " [   3    1    4    1    0   11    0   10   25    0    0    0    0    5\n",
      "     0    0    0    0    0    0    1   13    0    0    0    0    0]\n",
      " [   0    0    0    0    0   12    0    5    1    0    0   17    0    0\n",
      "     0    0    0    0    4    0    0    0    1    0    0    0    0]\n",
      " [   0    0    0    0    0    1    0   29   14    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    1    0    0    0    0    0]\n",
      " [   4    0    0    0    0   34    0   30   90    1    0    0    0  237\n",
      "     1    0    0    0    1    0    4   19    0    1    9    0    0]\n",
      " [   0    0    0    0    0   12    0   49    9    0    0    0    0    0\n",
      "    64    0    0    0    1    0    0    9    0    0    0    0    0]\n",
      " [   0    0    0    0    0    7    0    7   24    0    0    0    0    1\n",
      "     5    1    0    0    0    0    0    4    0    0    0    0    1]\n",
      " [   1    0    0    0    0    4    1    4   25    0    0    0    0    5\n",
      "     0    0    0    0    0    0    0    4    0    0   12    0    0]\n",
      " [   0    0    0    0    0    4    0   26    0    0    0    0    0    0\n",
      "     0    0    0    2    1    0    0    0    0    1    0    0    0]\n",
      " [   0    1    0    0    0   49    0   83   14    1    0    0    0    0\n",
      "     0    0    0    0   32    0    0   11    0    1    0    0    0]\n",
      " [   0    1    0    0    0   29    0    2  111    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    8    0    0    0    0    0]\n",
      " [   3    0    0    0    0   20    0   30   31    0    0    0    0   10\n",
      "     0    0    0    0    2    0   30   14    0    0    3    0    0]\n",
      " [   3    0    0    1    0  116    0  280  283    0    0    0    0   10\n",
      "     2    0    0    0    0    0    0  348    0    0    2    0    0]\n",
      " [   2    0    0    0    0   10    0   50    3    0    0    0    0    0\n",
      "     0    0    0    0    1    0    0    6   19    2    0    0    0]\n",
      " [   0    0    0    0    0   11    0   40    3    0    0    0    0    0\n",
      "     2    0    0    0    6    0    0    7    0   12    0    0    0]\n",
      " [  11    2    0    0    0   28    0   19  158    0    0    1    0   31\n",
      "     0    0    0    0    0    0    0   15    0    0   44    0    0]\n",
      " [   1    0    0    0    0    1    0    6    9    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    3    0    0    0    0    0]\n",
      " [   2    0    0    0    0   10    0    2   40    0    0    0    0    2\n",
      "     0    0    0    0    0    0    0    1    0    0    1    0  142]]\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      action        0.52      0.25      0.34       263\n",
      "       adult        0.73      0.21      0.33       112\n",
      "   adventure        0.45      0.15      0.23       139\n",
      "   animation        0.67      0.10      0.17       104\n",
      "   biography        0.00      0.00      0.00        61\n",
      "      comedy        0.51      0.58      0.54      1443\n",
      "       crime        0.43      0.03      0.05       107\n",
      " documentary        0.66      0.85      0.74      2659\n",
      "       drama        0.54      0.78      0.64      2697\n",
      "      family        0.40      0.07      0.11       150\n",
      "     fantasy        0.00      0.00      0.00        74\n",
      "   game-show        0.94      0.42      0.59        40\n",
      "     history        0.00      0.00      0.00        45\n",
      "      horror        0.64      0.55      0.59       431\n",
      "       music        0.63      0.44      0.52       144\n",
      "     musical        0.50      0.02      0.04        50\n",
      "     mystery        0.00      0.00      0.00        56\n",
      "        news        1.00      0.06      0.11        34\n",
      "  reality-tv        0.48      0.17      0.25       192\n",
      "     romance        0.00      0.00      0.00       151\n",
      "      sci-fi        0.58      0.21      0.31       143\n",
      "       short        0.47      0.33      0.39      1045\n",
      "       sport        0.63      0.20      0.31        93\n",
      "   talk-show        0.63      0.15      0.24        81\n",
      "    thriller        0.40      0.14      0.21       309\n",
      "         war        0.00      0.00      0.00        20\n",
      "     western        0.93      0.71      0.81       200\n",
      "\n",
      "     accuracy                           0.58     10843\n",
      "    macro avg       0.47      0.24      0.28     10843\n",
      " weighted avg       0.55      0.58      0.54     10843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(f\"Accuracy on Validation Set: {accuracy}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ea0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization for test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['DESCRIPTION'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17208895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "test_predictions = genre_model.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "134fad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the predictions to the test dataset\n",
    "test_df['PREDICTED_GENRE'] = test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7f5ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the test dataset with predictions\n",
    "output_file_path = 'test_data_solution.txt'\n",
    "test_df.to_csv(output_file_path, sep=',', index=False, header=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02b9f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content from the file and replace the comma with ':::'\n",
    "with open(output_file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read().replace(',', ':::')\n",
    "\n",
    "# Write the modified content back to the file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
